import pandas as pd
import zipfile
import requests
import io
import nltk
import streamlit as st
import numpy as np
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import plotly.graph_objects as go

# Baixar recursos necessÃ¡rios do NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')


# ----- FunÃ§Ãµes de Processamento e Modelagem -----
@st.cache_data
def carregar_dados():
    url = 'https://github.com/alexvaroz/data_science_alem_do_basico/raw/refs/heads/master/tweets_airlines.zip'
    response = requests.get(url)
    z = zipfile.ZipFile(io.BytesIO(response.content))
    csv_filename = [name for name in z.namelist() if name.endswith('.csv')][0]
    df = pd.read_csv(z.open(csv_filename))
    return df


def analisar_sentimento_textblob(text):
    analysis = TextBlob(str(text))
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity < 0:
        return 'negative'
    else:
        return 'neutral'


def treinar_tfidf_model(df):
    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['airline_sentiment'], test_size=0.2,
                                                        random_state=42)
    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train_vec, y_train)
    y_pred = model.predict(X_test_vec)

    cm = confusion_matrix(y_test, y_pred, labels=['positive', 'negative', 'neutral'])
    return y_test, y_pred, accuracy_score(y_test, y_pred), classification_report(y_test, y_pred, output_dict=True), cm


# ----- Interface do Streamlit -----
st.set_page_config(page_title="AnÃ¡lise de Sentimentos Comparativa", layout="wide")
st.title("âœˆï¸ AnÃ¡lise de Sentimentos com ComparaÃ§Ã£o de Modelos")

with st.expander("IntroduÃ§Ã£o"):
    st.markdown("""
   A anÃ¡lise de sentimentos Ã© uma tÃ©cnica poderosa utilizada para extrair opiniÃµes e emoÃ§Ãµes de textos, permitindo que as empresas compreendam melhor o feedback dos usuÃ¡rios e ajustem suas estratÃ©gias. Neste projeto, comparamos duas abordagens distintas para realizar a anÃ¡lise de sentimentos em tweets de companhias aÃ©reas:

TextBlob: Uma abordagem simples e intuitiva que utiliza a polaridade do texto para classificar os sentimentos como positivos, negativos ou neutros.

TF-IDF + RegressÃ£o LogÃ­stica: Uma abordagem mais robusta, que utiliza o TF-IDF (Term Frequency-Inverse Document Frequency) para extrair caracterÃ­sticas do texto e um modelo de regressÃ£o logÃ­stica para classificar os sentimentos.

AtravÃ©s dessa comparaÃ§Ã£o, nosso objetivo Ã© identificar qual modelo Ã© mais eficaz ao analisar sentimentos em tweets de companhias aÃ©reas. Para tanto, realizamos uma avaliaÃ§Ã£o detalhada de cada tÃ©cnica, apresentando as mÃ©tricas de desempenho e resultados de acurÃ¡cia, precisÃ£o, recall e F1-score, alÃ©m de uma anÃ¡lise mais aprofundada com grÃ¡ficos e tabelas.

Com isso, buscamos nÃ£o apenas comparar os modelos, mas tambÃ©m entender suas limitaÃ§Ãµes e potenciais, fornecendo insights valiosos sobre como escolher a melhor abordagem para a anÃ¡lise de sentimentos em diferentes contextos.
    """)


df = carregar_dados()
st.write("ðŸ” Amostra dos dados:")



st.dataframe(df[['text', 'airline_sentiment']].sample(5))

# ExplicaÃ§Ã£o geral das mÃ©tricas
with st.expander("â„¹ï¸MÃ©tricas de AvaliaÃ§Ã£o?"):
    st.markdown("""
    **Precision (PrecisÃ£o)**: Mede a acurÃ¡cia das previsÃµes feitas como **positivas**.
    - Quanto o modelo estÃ¡ correto quando prevÃª algo como positivo.

    **Recall (RevocaÃ§Ã£o ou Sensibilidade)**: Mede a capacidade do modelo de identificar **todos** os positivos reais.
    - Quanto o modelo consegue identificar todos os casos positivos de fato.

    **F1-Score**: Combina a precisÃ£o e o recall em uma Ãºnica mÃ©trica, equilibrando ambos.
    - Uma mÃ©dia ponderada entre a precisÃ£o e o recall. Uma boa mÃ©trica quando temos desequilÃ­brio entre as classes.

    **AcurÃ¡cia**: Percentual de acertos totais (todas as previsÃµes corretas divididas pelo total de previsÃµes).
    - A proporÃ§Ã£o de previsÃµes corretas feitas pelo modelo.

    **Macro avg**: MÃ©dia das mÃ©tricas (precisÃ£o, recall e F1) calculadas para cada classe, sem levar em consideraÃ§Ã£o o nÃºmero de exemplos em cada classe.

    **Weighted avg**: MÃ©dia ponderada das mÃ©tricas, considerando a proporÃ§Ã£o de cada classe no total de dados.

    Essas mÃ©tricas ajudam a entender **onde o modelo acerta mais** e **onde pode melhorar**. Elas sÃ£o importantes para avaliar a qualidade da previsÃ£o do modelo, especialmente quando as classes podem estar desequilibradas.
    """)

# --- TextBlob ---
st.header("ðŸ”  TextBlob")

with st.expander("â„¹ï¸ O que Ã© o TextBlob?"):
    st.markdown("""
    O **TextBlob** Ã© uma biblioteca simples e poderosa para anÃ¡lise de sentimentos. Ela atribui um valor de **polaridade** a cada texto. 
    - **Polaridade positiva**: Sentimentos positivos.
    - **Polaridade negativa**: Sentimentos negativos.
    - **Polaridade neutra**: Sentimentos neutros.

    Ele calcula a polaridade do texto e, com isso, conseguimos identificar rapidamente se o sentimento Ã© positivo, negativo ou neutro.
    """)

df['pred_textblob'] = df['text'].apply(analisar_sentimento_textblob)
acc_blob = accuracy_score(df['airline_sentiment'], df['pred_textblob'])
st.metric("ðŸŽ¯ AcurÃ¡cia (TextBlob)", f"{acc_blob * 100:.2f}%")

cm_blob = confusion_matrix(df['airline_sentiment'], df['pred_textblob'], labels=['positive', 'negative', 'neutral'])
acertos_por_classe_blob = {
    "positive": int(cm_blob[0, 0]),
    "negative": int(cm_blob[1, 1]),
    "neutral": int(cm_blob[2, 2])
}
erros_por_classe_blob = {
    "positive": int(cm_blob[0, 1] + cm_blob[0, 2]),
    "negative": int(cm_blob[1, 0] + cm_blob[1, 2]),
    "neutral": int(cm_blob[2, 0] + cm_blob[2, 1])
}

option = st.selectbox("Escolha o grÃ¡fico ou tabela", ["Tabela", "Pie Chart", "Bar Chart"])

cores_distintas = ["#0000ff", "#a1caf1", "#89a3dc"]

if option == "Tabela":
    st.subheader("Acertos e Erros por Classe (TextBlob):")
    dados_tabela = pd.DataFrame({
        "Classe": ["Positive", "Negative", "Neutral"],
        "Acertosâœ…": [acertos_por_classe_blob["positive"], acertos_por_classe_blob["negative"],
                    acertos_por_classe_blob["neutral"]],
        "ErrosâŒ": [erros_por_classe_blob["positive"], erros_por_classe_blob["negative"],
                  erros_por_classe_blob["neutral"]]
    })
    st.write(dados_tabela)

elif option == "Pie Chart":
    labels = ['Positive', 'Negative', 'Neutral']
    sizes = [acertos_por_classe_blob["positive"], acertos_por_classe_blob["negative"],
             acertos_por_classe_blob["neutral"]]

    fig_pie = go.Figure(data=[go.Pie(labels=labels, values=sizes, hole=0.4,
                                     marker=dict(colors=cores_distintas))])
    fig_pie.update_layout(title="DistribuiÃ§Ã£o de Acertos por Classe (TextBlob)", margin=dict(t=20, b=20, l=20, r=20),
                          font=dict(size=10))
    st.plotly_chart(fig_pie, use_container_width=True)

elif option == "Bar Chart":
    fig_bar = go.Figure([go.Bar(x=list(acertos_por_classe_blob.keys()), y=list(acertos_por_classe_blob.values()),
                                marker=dict(color=cores_distintas))])
    fig_bar.update_layout(title="Acertos por Classe (TextBlob)", xaxis_title="Classe", yaxis_title="NÃºmero de Acertos",
                          font=dict(size=10))
    st.plotly_chart(fig_bar, use_container_width=True)

# RelatÃ³rio como tabela (TextBlob)
with st.expander("ðŸ“‹ RelatÃ³rio TextBlob"):
    report_blob = classification_report(df['airline_sentiment'], df['pred_textblob'], output_dict=True)
    df_report_blob = pd.DataFrame(report_blob).transpose()
    st.dataframe(df_report_blob.style.format({"precision": "{:.2f}", "recall": "{:.2f}", "f1-score": "{:.2f}", "support": "{:.0f}"}))

with st.expander("ðŸ“‹ InterpretaÃ§Ã£o dos Resultados"):
    st.markdown("""

#### Classe **Positive**:
- O modelo acertou **1807** tweets classificados como positivos e errou **556**. A quantidade de acertos nesta classe sugere que o modelo tem uma boa performance para identificar sentimentos positivos, mas ainda com um nÃºmero significativo de erros.

#### Classe **Negative**:
- Para sentimentos negativos, o modelo acertou **3235** tweets, mas errou **5943**. Esse nÃºmero de erros Ã© maior em comparaÃ§Ã£o com as outras classes, o que indica que o modelo tem mais dificuldade em classificar corretamente tweets negativos. Isso pode sugerir a necessidade de ajustes no modelo ou mais dados de treinamento para melhorar a precisÃ£o em sentimentos negativos.

#### Classe **Neutral**:
- O modelo acertou **1757** tweets neutros e errou **1342**. A quantidade de acertos nesta classe Ã© razoÃ¡vel, mas o nÃºmero de erros tambÃ©m Ã© significativo. Isso indica que o modelo pode ter dificuldade em distinguir sentimentos neutros dos outros tipos de sentimentos (positivos ou negativos).

### O que isso nos diz?
- **Desbalanceamento de Dados**: Se o nÃºmero de erros for muito maior em uma classe especÃ­fica, isso pode ser um indicativo de desbalanceamento nas classes ou de que o modelo nÃ£o estÃ¡ conseguindo capturar bem as caracterÃ­sticas dessa classe. O nÃºmero de erros altos para a classe **Negative**, por exemplo, sugere que o modelo pode estar mais propenso a classificar erroneamente tweets negativos como neutros ou atÃ© mesmo positivos.

- **Ajustes no Modelo**: As classes **Negative** e **Neutral** parecem ter uma quantidade maior de erros em comparaÃ§Ã£o com a classe **Positive**. Isso pode indicar que o modelo tem uma tendÃªncia a acertar mais os sentimentos positivos, mas tem dificuldades para discernir sentimentos negativos ou neutros. Talvez seja necessÃ¡rio ajustar o treinamento, com mais dados ou tÃ©cnicas de balanceamento de classes.

- **MÃ©tricas Importantes**: O nÃºmero de acertos e erros tambÃ©m afeta as mÃ©tricas como **PrecisÃ£o**, **Recall** e **F1-Score**, que sÃ£o essenciais para medir o desempenho de um modelo em cenÃ¡rios com classes desbalanceadas. A precisÃ£o pode ser impactada negativamente se houver muitos erros, especialmente nas classes com menor representaÃ§Ã£o, enquanto o recall pode ser baixo para as classes com maior nÃºmero de erros.

### PossÃ­veis Melhorias:
1. **Ajuste no modelo**: Melhorar o modelo de classificaÃ§Ã£o, considerando ajustes de parÃ¢metros ou tÃ©cnicas como **SMOTE (Synthetic Minority Over-sampling Technique)** para balancear os dados de treinamento.
2. **AnÃ¡lise de erros**: Analisar os erros cometidos para entender se hÃ¡ padrÃµes nos tweets que sÃ£o classificados incorretamente, como a presenÃ§a de palavras ambÃ­guas ou ironia que podem afetar a classificaÃ§Ã£o.
3. **Mais dados de treinamento**: Coletar mais dados, especialmente para as classes com menos acertos (como **Negative**), pode ajudar a melhorar o desempenho do modelo.
 """)

# --- TF-IDF + Logistic Regression ---
st.header("ðŸ“Š TF-IDF + Logistic Regression")

# ExplicaÃ§Ã£o sobre o mÃ©todo TF-IDF + Logistic Regression
with st.expander("â„¹ï¸ O que Ã© TF-IDF + Logistic Regression?"):
    st.markdown("""
    O **TF-IDF (Term Frequency-Inverse Document Frequency)** Ã© uma tÃ©cnica que transforma o texto em nÃºmeros, dando mais peso a palavras importantes.
    Em seguida, o modelo de **RegressÃ£o LogÃ­stica** usa esses nÃºmeros para fazer previsÃµes. Ele Ã© um classificador, ou seja, ele tenta adivinhar a categoria (positivo/negativo) de um tweet com base nas palavras.

    - **TF-IDF**: Transforma o texto em nÃºmeros.
    - **RegressÃ£o LogÃ­stica**: Classifica o sentimento com base nos nÃºmeros gerados pelo TF-IDF.
    """)


y_test_tfidf, y_pred_tfidf, acc_tfidf, report_tfidf_dict, cm_tfidf = treinar_tfidf_model(df)
st.metric("ðŸŽ¯ AcurÃ¡cia (TF-IDF)", f"{acc_tfidf * 100:.2f}%")

acertos_por_classe_tfidf = {
    "positive": int(cm_tfidf[0, 0]),
    "negative": int(cm_tfidf[1, 1]),
    "neutral": int(cm_tfidf[2, 2])
}
erros_por_classe_tfidf = {
    "positive": int(cm_tfidf[0, 1] + cm_tfidf[0, 2]),
    "negative": int(cm_tfidf[1, 0] + cm_tfidf[1, 2]),
    "neutral": int(cm_tfidf[2, 0] + cm_tfidf[2, 1])
}

option_tfidf = st.selectbox("Escolha o grÃ¡fico ou tabela para TF-IDF", ["Tabela", "Pie Chart", "Bar Chart"])

if option_tfidf == "Tabela":
    st.subheader("Acertos e Erros por Classe (TF-IDF):")
    dados_tabela_tfidf = pd.DataFrame({
        "Classe": ["Positive", "Negative", "Neutral"],
        "Acertosâœ…": [acertos_por_classe_tfidf["positive"], acertos_por_classe_tfidf["negative"],
                    acertos_por_classe_tfidf["neutral"]],
        "ErrosâŒ": [erros_por_classe_tfidf["positive"], erros_por_classe_tfidf["negative"],
                  erros_por_classe_tfidf["neutral"]]
    })
    st.write(dados_tabela_tfidf)

elif option_tfidf == "Pie Chart":
    sizes_tfidf = [acertos_por_classe_tfidf["positive"], acertos_por_classe_tfidf["negative"],
                   acertos_por_classe_tfidf["neutral"]]

    fig_pie_tfidf = go.Figure(data=[go.Pie(labels=labels, values=sizes_tfidf, hole=0.4,
                                           marker=dict(colors=cores_distintas))])
    fig_pie_tfidf.update_layout(title="DistribuiÃ§Ã£o de Acertos por Classe (TF-IDF)",
                                margin=dict(t=20, b=20, l=20, r=20), font=dict(size=10))
    st.plotly_chart(fig_pie_tfidf, use_container_width=True)

elif option_tfidf == "Bar Chart":
    fig_bar_tfidf = go.Figure(
        [go.Bar(x=list(acertos_por_classe_tfidf.keys()), y=list(acertos_por_classe_tfidf.values()),
                marker=dict(color=cores_distintas))])
    fig_bar_tfidf.update_layout(title="Acertos por Classe (TF-IDF)", xaxis_title="Classe",
                                yaxis_title="NÃºmero de Acertos", font=dict(size=10))
    st.plotly_chart(fig_bar_tfidf, use_container_width=True)

# RelatÃ³rio como tabela (TF-IDF)
with st.expander("ðŸ“‹ RelatÃ³rio TF-IDF"):
    df_report_tfidf = pd.DataFrame(report_tfidf_dict).transpose()
    st.dataframe(df_report_tfidf.style.format({"precision": "{:.2f}", "recall": "{:.2f}", "f1-score": "{:.2f}", "support": "{:.0f}"}))
with st.expander("ðŸ“‹ InterpretaÃ§Ã£o dos Resultados"):
    st.markdown("""


#### Classe **Positive**:
- O modelo acertou **289** tweets classificados como positivos e errou **170**. Embora o nÃºmero de acertos seja significativo, a quantidade de erros tambÃ©m Ã© alta, indicando que o modelo pode ter dificuldade em classificar corretamente alguns sentimentos positivos. Essa discrepÃ¢ncia pode ocorrer devido Ã  presenÃ§a de palavras ambÃ­guas ou a complexidade do modelo.

#### Classe **Negative**:
- Para sentimentos negativos, o modelo acertou **1758** tweets, mas errou apenas **131**. O modelo parece ter um bom desempenho em classificar sentimentos negativos, com um nÃºmero muito baixo de erros. Isso sugere que a caracterÃ­stica dos tweets negativos estÃ¡ bem representada no treinamento e que o modelo consegue capturar adequadamente os sentimentos negativos.

#### Classe **Neutral**:
- O modelo acertou **283** tweets neutros e errou **297**. Apesar de um nÃºmero considerÃ¡vel de erros, o modelo acertou uma quantidade razoÃ¡vel de tweets neutros. No entanto, os erros nesta classe indicam que o modelo pode estar confundindo tweets neutros com os sentimentos positivos ou negativos, o que Ã© comum em modelos que analisam textos curtos.

### O que isso nos diz?
- **Desbalanceamento de Dados**: O modelo parece ter uma boa performance para a classe **Negative**, mas com dificuldades para identificar corretamente os sentimentos **Positive** e **Neutral**. Isso pode ser um indicativo de desbalanceamento no nÃºmero de exemplos dessas classes no conjunto de treinamento ou de uma complexidade maior ao tentar distinguir os sentimentos neutros e positivos.

- **Ajustes no Modelo**: A classe **Neutral** apresenta um nÃºmero relativamente alto de erros. Isso sugere que o modelo pode ser aprimorado com mais dados ou por meio de tÃ©cnicas de balanceamento, como **SMOTE**, para melhorar a detecÃ§Ã£o de sentimentos neutros.

- **MÃ©tricas Importantes**: A precisÃ£o em sentimentos **Negative** parece ser muito boa, enquanto a acurÃ¡cia de **Positive** e **Neutral** pode ser melhorada. Isso pode impactar diretamente mÃ©tricas como **F1-Score**, **Recall** e **PrecisÃ£o**, que precisam ser avaliadas para entender melhor a eficÃ¡cia do modelo em diferentes classes.

### PossÃ­veis Melhorias:
1. **ReforÃ§ar a classificaÃ§Ã£o para Neutral**: Uma soluÃ§Ã£o pode ser o uso de **tÃ©cnicas de aumento de dados** (data augmentation) ou a reavaliaÃ§Ã£o dos exemplos de treinamento da classe neutra, que podem ter caracterÃ­sticas semelhantes a outras classes.
2. **Ajuste nos parÃ¢metros do TF-IDF**: A utilizaÃ§Ã£o de parÃ¢metros como **max_features** e **ngram_range** pode melhorar a captura de caracterÃ­sticas especÃ­ficas de cada classe.
3. **Balanceamento de Dados**: TÃ©cnicas como **SMOTE** ou **undersampling** para as classes desbalanceadas podem ser aplicadas para melhorar a performance nas classes com maior nÃºmero de erros.
 """)

import plotly.graph_objects as go

# ComparaÃ§Ã£o de acurÃ¡cia
st.header("ðŸ“ˆ ComparaÃ§Ã£o de AcurÃ¡cia entre os modelos")

# Criar DataFrame para comparaÃ§Ã£o de acurÃ¡cia
acuracia_df = pd.DataFrame({
    "Modelos": ["TextBlob", "TF-IDF + LR"],
    "AcurÃ¡cia": [acc_blob, acc_tfidf]
})

# Criar grÃ¡fico com Plotly
fig = go.Figure()

# Adicionar a linha para a acurÃ¡cia
fig.add_trace(go.Scatter(
    x=acuracia_df["Modelos"],
    y=acuracia_df["AcurÃ¡cia"],
    mode='lines+markers',
    name='AcurÃ¡cia',
    marker=dict(color='blue', size=10),  # Personalizando o marcador
    line=dict(color='blue', width=2)     # Personalizando a linha
))

# Personalizar layout do grÃ¡fico
fig.update_layout(
    xaxis_title="Modelos",
    yaxis_title="AcurÃ¡cia (%)",
    yaxis=dict(range=[0, 1]),  # Definir intervalo do eixo Y de 0 a 1 (ou de 0% a 100%)
    template="plotly_dark",  # Tema para o grÃ¡fico
    font=dict(size=12),
    showlegend=False  # Remover legenda
)

# Exibir o grÃ¡fico no Streamlit
st.plotly_chart(fig)


# ConclusÃ£o
st.header("ðŸ”š ConclusÃ£o")
st.markdown(f"""
Com base nos resultados da anÃ¡lise de desempenho dos modelos de TextBlob e TF-IDF + RegressÃ£o LogÃ­stica, podemos observar uma diferenÃ§a significativa na eficÃ¡cia de cada abordagem.

TextBlob obteve uma acurÃ¡cia de 46.44%, o que indica um desempenho relativamente baixo em relaÃ§Ã£o Ã  tarefa de anÃ¡lise de sentimentos. Embora o TextBlob seja uma ferramenta simples e fÃ¡cil de usar para anÃ¡lise de sentimentos, seu desempenho pode ser afetado por sua abordagem mais bÃ¡sica, que nÃ£o considera de forma tÃ£o aprofundada as nuances do texto.

Por outro lado, o modelo TF-IDF + RegressÃ£o LogÃ­stica alcanÃ§ou uma acurÃ¡cia de 79.58%, um desempenho muito superior. O TF-IDF Ã© uma tÃ©cnica que captura a importÃ¢ncia relativa das palavras em um documento, enquanto a RegressÃ£o LogÃ­stica Ã© um classificador robusto, especialmente quando combinado com representaÃ§Ãµes textuais como o TF-IDF. Essa combinaÃ§Ã£o permite ao modelo capturar melhor as relaÃ§Ãµes semÃ¢nticas entre as palavras e suas contribuiÃ§Ãµes para a classificaÃ§Ã£o de sentimentos.

ConclusÃ£o Final: A anÃ¡lise sugere que o modelo TF-IDF + RegressÃ£o LogÃ­stica Ã© o mais eficaz para a tarefa de anÃ¡lise de sentimentos, apresentando uma acurÃ¡cia significativamente maior que o TextBlob. Isso destaca a importÃ¢ncia de tÃ©cnicas mais sofisticadas de representaÃ§Ã£o textual e de aprendizado de mÃ¡quina, como o TF-IDF, para melhorar a precisÃ£o das previsÃµes em tarefas de classificaÃ§Ã£o de sentimentos.
""")
